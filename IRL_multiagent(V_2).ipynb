{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q6HStaN_aUYP"
      },
      "outputs": [],
      "source": [
        "# @title Install requirements\n",
        "from IPython.display import clear_output\n",
        "# @markdown We install the acme library, see [here](https://github.com/deepmind/acme) for more info.\n",
        "\n",
        "# @markdown **WARNING:** There may be *errors* and/or *warnings* reported during the installation. However, they should be ignored.\n",
        "!pip install --upgrade pip --quiet\n",
        "!pip install imageio --quiet\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install gym --quiet\n",
        "!pip install enum34 --quiet\n",
        "!pip install dm-env --quiet\n",
        "!pip install pandas --quiet\n",
        "!pip install grpcio==1.34.0 --quiet\n",
        "!pip install tensorflow --quiet\n",
        "!pip install typing --quiet\n",
        "!pip install einops --quiet\n",
        "!pip install dm-acme --quiet\n",
        "!pip install dm-acme[reverb] --quiet\n",
        "!pip install dm-acme[jax,tensorflow] --quiet\n",
        "!pip install dm-acme[envs] --quiet\n",
        "!pip install dm-env --quiet\n",
        "\n",
        "!pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet\n",
        "from evaltools.airtable import AirtableForm\n",
        "\n",
        "# Generate airtable form\n",
        "atform = AirtableForm('appn7VdPRseSoMXEG','W3D2_T1','https://portal.neuromatchacademy.org/api/redirect/to/3e77471d-4de0-4e43-a026-9cfb603b5197')\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import modules\n",
        "# Import modules\n",
        "import gym\n",
        "import enum\n",
        "import copy\n",
        "import time\n",
        "import acme\n",
        "import torch\n",
        "import base64\n",
        "import dm_env\n",
        "import IPython\n",
        "import imageio\n",
        "import warnings\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import scipy\n",
        "from scipy.optimize import linprog\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme.utils import tree_utils\n",
        "from acme.utils import loggers\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "from typing import Callable, Sequence\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.set_printoptions(precision=3, suppress=1)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jfk0pYW9a1yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating test_data "
      ],
      "metadata": {
        "id": "U9xDYsEeYJYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title test_data\n",
        "data = {'1':{'F':'2'}, '2':{'F':'3', 'B':'1'}, '3':{'F':'4', 'B':'2', 'T':'23'}, '4':{'B':'3'},\n",
        "        '21':{'F':'22'}, '22':{'F':'23', 'B':'21'}, '23':{'F':'24', 'B':'21', 'T':'3'}, '24':{'B':'23'}\n",
        "        }\n",
        "distance = { ('1','2'):70, ('2','3'):50, ('3','4'):70, ('21','22'):50, ('22','23'):50, ('23','24'):50, ('3','23'):0,\n",
        "             ('2','1'):70, ('3','2'):50, ('4','3'):70, ('22','21'):50, ('23','21'):50, ('24','23'):50, ('23','3'):0\n",
        "            }\n",
        "q = {('1','F'): 0,('2','F'): 0,('2','B'): 0, ('3','F'): 0,('3','B'): 0, ('3','T'): 0,('4','B'): 0,\n",
        "     ('21','F'): 0,('22','F'): 0,('22','B'): 0, ('23','F'): 0,('23','B'): 0, ('23','T'): 0,('24','B'): 0}\n"
      ],
      "metadata": {
        "id": "YtgRIOM6a04u",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title test_data\n",
        "data = {'1':{'F':'2','F2':'21'}, '2':{'F':'3', 'B':'1', 'T':'22'}, '3':{'F':'4', 'B':'2', 'T':'23'}, '4':{'B':'3','F':'24'},\n",
        "        '21':{'F':'22','B2':'1'}, '22':{'F':'23', 'B':'21', 'T':'2'}, '23':{'F':'24', 'B':'22', 'T':'3'}, '24':{'B':'23'}\n",
        "        }\n",
        "distance = { ('1','2'):70, ('2','3'):70, ('3','4'):70, ('21','22'):50, ('22','23'):40, ('23','24'):50, ('3','23'):0, ('2','22'):0, ('4','24'):500, ('1','21'):560, \n",
        "             ('2','1'):70, ('3','2'):70, ('4','3'):70, ('22','21'):50, ('23','22'):40, ('24','23'):50, ('23','3'):0, ('22','2'):0, ('24','4'):500, ('21','1'):560\n",
        "}   "
      ],
      "metadata": {
        "id": "kQBFT1DAg36V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Environment 1\n",
        "class Environment:\n",
        "  \n",
        "  def __init__(self, dt,dc):\n",
        "    self.x = dt\n",
        "    self.distance = dc\n",
        "    self.alpha = [1,1]\n",
        "\n",
        "  def Possible_actions(self,s):\n",
        "    p_actions = list(self.x[s].keys())\n",
        "    self.p_actions = p_actions\n",
        "    return p_actions\n",
        "    \n",
        "  def next_step(self,s,a):\n",
        "    n_step = self.x[s][a] \n",
        "    return n_step\n",
        "  \n",
        "  def Reward(self,s,a):\n",
        "    reward = 0\n",
        "    t_reward = 0\n",
        "    # ns = self.n_step\n",
        "    w1 =1\n",
        "    w2 =1\n",
        "\n",
        "    if a == 'T':\n",
        "      t_reward += -200\n",
        "    else:\n",
        "      reward += (-1) * self.distance[(s, self.next_step(s,a))]\n",
        "\n",
        "    # total_reward = w1 * reward + w2 * t_reward\n",
        "\n",
        "    r= [reward,t_reward]\n",
        "    total_reward = np.dot(self.alpha,r)\n",
        "\n",
        "    if self.next_step(s,a) == '24':\n",
        "      total_reward += 1000\n",
        "    \n",
        "    # total_reward = w1 * reward + w2 * t_reward\n",
        "    return float(total_reward)\n",
        "    \n",
        "\n",
        "  def Reward_features(self,s,a):\n",
        "    reward = 0\n",
        "    t_reward = 0\n",
        "    # ns = self.n_step\n",
        "    w1 =1\n",
        "    w2 =1\n",
        "\n",
        "    if a == 'T':\n",
        "      t_reward += -200\n",
        "    else:\n",
        "      reward += (-1) * self.distance[(s, self.next_step(s,a))]\n",
        "\n",
        "    return (reward, t_reward)\n",
        "\n",
        "\n",
        "\n",
        "env = Environment(data,distance)"
      ],
      "metadata": {
        "id": "ekdOdiweallg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Environment 2\n",
        "class Environment2:\n",
        "  \n",
        "  def __init__(self, dt,dc):\n",
        "    self.x = dt\n",
        "    self.distance = dc\n",
        "    self.alpha = [1,1]\n",
        "\n",
        "  def Possible_actions(self,s):\n",
        "    p_actions = list(self.x[s].keys())\n",
        "    self.p_actions = p_actions\n",
        "    return p_actions\n",
        "    \n",
        "  def next_step(self,s,a):\n",
        "    n_step = self.x[s][a] \n",
        "    return n_step\n",
        "  \n",
        "  def Reward(self,s,a):\n",
        "    reward = 0\n",
        "    t_reward = 0\n",
        "    # ns = self.n_step\n",
        "    w1 =1\n",
        "    w2 =1\n",
        "\n",
        "    if a == 'T':\n",
        "      t_reward += -200\n",
        "    else:\n",
        "      reward += (-1) * self.distance[(s, self.next_step(s,a))]\n",
        "\n",
        "    # total_reward = w1 * reward + w2 * t_reward\n",
        "\n",
        "    r= [reward,t_reward]\n",
        "    total_reward = np.dot(self.alpha,r)\n",
        "\n",
        "    if self.next_step(s,a) == '22':\n",
        "      total_reward += 1000\n",
        "    \n",
        "    # total_reward = w1 * reward + w2 * t_reward\n",
        "    return float(total_reward)\n",
        "    \n",
        "\n",
        "  def Reward_features(self,s,a):\n",
        "    reward = 0\n",
        "    t_reward = 0\n",
        "    # ns = self.n_step\n",
        "    w1 =1\n",
        "    w2 =1\n",
        "\n",
        "    if a == 'T':\n",
        "      t_reward += -200\n",
        "    else:\n",
        "      reward += (-1) * self.distance[(s, self.next_step(s,a))]\n",
        "\n",
        "    return (reward, t_reward)\n",
        "\n",
        "\n",
        "\n",
        "env_2 = Environment2(data,distance)"
      ],
      "metadata": {
        "id": "ePirarMDonuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Epsilon_greedy_ function\n",
        "def epsilon_greedy(qvalue,state):\n",
        "  epsilon = 0.4\n",
        "  e =np.random.random()\n",
        "\n",
        "  if e > epsilon:\n",
        "    action = max(qvalue[state],key = qvalue[state].get)\n",
        "  else :\n",
        "    action = np.random.choice(list(qvalue[state].keys()))\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "def greedy(qvalue,state):\n",
        "\n",
        "  action = max(qvalue[state],key = qvalue[state].get)\n",
        "  return action\n"
      ],
      "metadata": {
        "id": "VK6Ohw73csmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Agent Original\n",
        "class Agent():\n",
        "  def __init__(self, environment):\n",
        "    self.envv = environment\n",
        "    self.state = None\n",
        "  \n",
        "    self.q = {'1':{'F':0,'F2':0}, '2':{'F':0, 'B':0, 'T':0}, '3':{'F':0, 'B':0, 'T':0}, '4':{'B':0,'F':0},\n",
        "        '21':{'F':0, 'B2':0}, '22':{'F':0, 'B':0, 'T':0}, '23':{'F':0, 'B':0, 'T':0}, '24':{'B':0}}\n",
        "    \n",
        "    self.p = {'1':'F', '2':'F', '3':'F', '4':'B',\n",
        "        '21':'F', '22':'F', '23':'F', '24':'B'}\n",
        "\n",
        "  def select_action(self,s):\n",
        "    p_actions = self.envv.Possible_actions(s)\n",
        "    action = epsilon_greedy(self.q,s)\n",
        "\n",
        "    return action\n",
        "\n",
        "  def select_greedy_action(self,s):\n",
        "    p_actions = self.envv.Possible_actions(s)\n",
        "    action = greedy(self.q,s)\n",
        "\n",
        "    return action\n",
        "\n",
        "  def observe(self,s):\n",
        "    self.state = s\n",
        "    self.a = self.select_action(s)\n",
        "    reward = self.envv.Reward(self.state, self.a)\n",
        "    g = 0.99\n",
        "    next_s = self.envv.next_step(self.state,self.a)\n",
        "    next_a = self.select_greedy_action(next_s)\n",
        "    self._td_error = reward + (g * (self.q[next_s][next_a] - self.q[self.state][self.a]))\n",
        "    \n",
        "    return self._td_error\n",
        "\n",
        "  def update(self):\n",
        "\n",
        "    ## step_size = 0.1\n",
        "    self.q[self.state][self.a] += int(0.1 * self._td_error)\n",
        "\n",
        "    self.state = self.envv.next_step(self.state,self.a)\n",
        "    return self.state\n",
        "  \n",
        "  def qvalue(self):\n",
        "    return self.q\n",
        "  \n",
        "  def policy(self):\n",
        "    for i in self.q.keys():\n",
        "      self.p[i] = self.select_greedy_action(i)\n",
        "    return self.p\n",
        "\n",
        "  \n",
        "agent = Agent(env)\n",
        "agent_2 = Agent(env_2)"
      ],
      "metadata": {
        "id": "3ahnyfXCFLaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loop Agent_1\n",
        "def loop(environment, agent, initial_state, goal_state):\n",
        "  \n",
        "  episode = 0\n",
        "  while episode <= 1000:\n",
        "    step = 0\n",
        "    reward_t = 0\n",
        "    state = '1'\n",
        "    goal = goal_state\n",
        "    route =['1']\n",
        "\n",
        "    while state != goal:\n",
        "\n",
        "      agent.select_action(state)\n",
        "      reward_t += agent.observe(state)\n",
        "      state = agent.update()\n",
        "      step += 1\n",
        "      route.append(state)\n",
        "\n",
        "    episode += 1\n",
        " \n",
        "\n",
        "  return route\n",
        "\n",
        "loop(env,agent,'1','24')\n",
        "print(agent.qvalue())\n",
        "print(agent.policy())\n",
        "## loop for agent_2\n",
        "loop(env_2,agent_2,'1','22')\n",
        "print(agent_2.qvalue())\n",
        "print(agent_2.policy())"
      ],
      "metadata": {
        "id": "NmAJTALhXpQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Route policy function"
      ],
      "metadata": {
        "id": "G0Xsgdb_YaR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def route_policy(a,b,policy_,environment):\n",
        "  route = [a]\n",
        "  s = a\n",
        "  while s != b:\n",
        "    s = environment.next_step(s,policy_[s])\n",
        "    route.append(s)\n",
        "  return route"
      ],
      "metadata": {
        "id": "C5pgR10tHITi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(route_policy('1','24',agent.policy(),env))\n",
        "p = {'1':'F', '2':'F', '3':'F', '4':'F',\n",
        "        '21':'F', '22':'F', '23':'F', '24':'B'}\n",
        "print(route_policy('1','24', p, env))\n",
        "\n",
        "p2 = {'1':'F', '2':'T', '3':'F', '4':'F',\n",
        "        '21':'F', '22':'F', '23':'F', '24':'B'}\n",
        "print(route_policy('1','22', agent_2.policy(), env_2))\n",
        "print(route_policy('1','22', p2, env_2))"
      ],
      "metadata": {
        "id": "WfkYYiDxKkdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.alpha = [3,8]\n",
        "agent = Agent(env)\n",
        "loop(env,agent,'1','24')\n",
        "print(route_policy('1','24',agent.policy(),env))\n",
        "p = {'1':'F', '2':'F', '3':'F', '4':'F',\n",
        "        '21':'F', '22':'F', '23':'F', '24':'B'}\n",
        "print(route_policy('1','24', p, env))"
      ],
      "metadata": {
        "id": "KcE5K7_hKo-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value function generator"
      ],
      "metadata": {
        "id": "b9OvY6-WYhcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valuefunctiongenerator(alpha_vec, policy_,environment_, origin, destination):  \n",
        "  v={'d':0, 't':0}\n",
        "  route = route_policy(origin,destination, policy_, environment_)\n",
        "  \n",
        "  i=1\n",
        "  for s in route[0:-1]:\n",
        "    (d,t) = environment_.Reward_features(s,policy_[s])\n",
        "\n",
        "    v['d'] +=  d\n",
        "    v['t'] +=  t\n",
        "    i += 1\n",
        "\n",
        "  V_basis = np.array([v['d'],v['t']])\n",
        "  #alpha = [1,1]\n",
        "  V = np.dot(alpha_vec, V_basis)\n",
        "\n",
        "  return V, V_basis\n"
      ],
      "metadata": {
        "id": "ubzfxPyZNMKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Agent_1\n",
        "print(valuefunctiongenerator([1,1],agent.policy(),env, '1', '24'))\n",
        "print(valuefunctiongenerator([1,1],p,env,'1', '24'))\n",
        "## Agent_2\n",
        "print(valuefunctiongenerator([1,1],agent_2.policy(),env_2, '1', '22'))\n",
        "print(valuefunctiongenerator([1,1],p2,env_2,'1', '22'))"
      ],
      "metadata": {
        "id": "MNTTe1U8OQ1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inverse reinforcment learning (MaxEntropy approach)"
      ],
      "metadata": {
        "id": "SB6nREXVYr4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IRL_MaxEnt(alpha_vec, expert_policy, expert_policy_2):\n",
        "  learning_rate = 0.01\n",
        "  V_input = np.zeros(2)\n",
        "  penalty_factor = 2\n",
        "  env.alpha = alpha_vec\n",
        "  ## valufunction of expert policy for agent_1\n",
        "  # V_expert, V_basis_expert = valuefunctiongenerator(alpha_vec, expert_policy, env, '1', '24')\n",
        "  ## valufunction of expert policy for agent_2\n",
        "  # V_expert_2, V_basis_expert_2 = valuefunctiongenerator(alpha_vec, expert_policy_2, env_2, '1', '22')\n",
        "  \n",
        "  npp=0\n",
        "  n=1000\n",
        "  pbar = tqdm(desc = 'while loop',total = n,position=0, leave=True)\n",
        "  while npp < n:\n",
        "    # print(npp)\n",
        "\n",
        "    if npp ==0 :\n",
        "      rl_policy = {'1':'F', '2':'T', '3':'T', '4':'F',\n",
        "          '21':'F', '22':'F', '23':'F', '24':'B'}\n",
        "      rl_policy_2 = {'1':'F2', '2':'T', '3':'T', '4':'F',\n",
        "          '21':'F', '22':'F', '23':'F', '24':'B'}\n",
        "    \n",
        "    ## value function of RL policy\n",
        "    ## Agent_1\n",
        "    V_rl, V_basis_rl = valuefunctiongenerator(alpha_vec, rl_policy, env, '1', '24')\n",
        "    ## Agent_2\n",
        "    V_rl_2, V_basis_rl_2 = valuefunctiongenerator(alpha_vec, rl_policy_2, env_2, '1', '22')\n",
        "\n",
        "    ## valufunction of expert policy for agent_1\n",
        "    V_expert, V_basis_expert = valuefunctiongenerator(alpha_vec, expert_policy, env, '1', '24')\n",
        "    ## valufunction of expert policy for agent_2\n",
        "    V_expert_2, V_basis_expert_2 = valuefunctiongenerator(alpha_vec, expert_policy_2, env_2, '1', '22')\n",
        "    \n",
        "\n",
        "    if V_expert - V_rl >= 0:\n",
        "      o=0\n",
        "      if V_expert_2 - V_rl_2 < 0:\n",
        "        learning_rate = 0.001\n",
        "        gradient = V_basis_expert_2 - V_basis_rl_2\n",
        "        delta = learning_rate * gradient\n",
        "      else:\n",
        "        print('Done')\n",
        "        break\n",
        "\n",
        "    else:\n",
        "      gradient = V_basis_expert - V_basis_rl\n",
        "      delta = learning_rate * gradient\n",
        "    ## optimization problem  \n",
        "    #res = scipy.optimize.linprog(V_input, bounds=(-1,1), method=\"simplex\")\n",
        "    #print('V_input',V_input)\n",
        "    #res = linprog(V_input, bounds=(0,1), method=\"simplex\")\n",
        "    #res = linprog(V_input.tolist(), A_ub=[[V_input[0],V_input[1]]], b_ub=0, bounds=(0,1), method=\"simplex\")\n",
        "    if alpha_vec[0] + delta[0] < 0:\n",
        "      delta[0] = 0 - alpha_vec[0]/2\n",
        "\n",
        "    alpha_vec_new = alpha_vec + delta\n",
        "    env.alpha = alpha_vec_new\n",
        "    env_2.alpha = alpha_vec_new\n",
        "    alpha_vec = alpha_vec_new\n",
        "    #run the RL for finding the new policy\n",
        "    agent = Agent(env)\n",
        "    loop(env,agent,'1','24')\n",
        "    rl_policy = agent.policy()\n",
        "\n",
        "  \n",
        "\n",
        "    agent_2 = Agent(env_2)\n",
        "    loop(env_2,agent_2,'1','22')\n",
        "    rl_policy_2 = agent_2.policy()\n",
        "    \n",
        "    #alpha_vec_new = res.x\n",
        "    #V_expert_new = np.dot(alpha_vec_new, V_basis_expert)\n",
        "    npp += 1\n",
        "    pbar.update(1)\n",
        "\n",
        "\n",
        "  print(agent.policy())\n",
        "  print(route_policy('1','24',agent.policy(),env))\n",
        "  print(env.alpha)\n",
        "  print(agent_2.policy())\n",
        "  print(route_policy('1','22',agent_2.policy(),env_2))\n",
        "  print(env_2.alpha)\n",
        "  pbar.close()"
      ],
      "metadata": {
        "id": "VbVzO0psalE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IRL_MaxEnt([5,5], p, p2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lx_uTWHZrsm",
        "outputId": "0e68ddd8-2cec-4083-f99a-03982427014f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "while loop:  10%|â–‰         | 96/1000 [00:30<04:51,  3.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n",
            "{'1': 'F', '2': 'F', '3': 'F', '4': 'F', '21': 'F', '22': 'F', '23': 'F', '24': 'B'}\n",
            "['1', '2', '3', '4', '24']\n",
            "[2.57 6.8 ]\n",
            "{'1': 'F', '2': 'T', '3': 'B', '4': 'B', '21': 'F', '22': 'F', '23': 'B', '24': 'B'}\n",
            "['1', '2', '22']\n",
            "[2.57 6.8 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title IRL_linear programming\n",
        "def IRL_LP(alpha_vec, expert_policy):\n",
        "\n",
        "  V_input = np.zeros(2)\n",
        "  penalty_factor = 2\n",
        "  ## valufunction of expert policy\n",
        "  V_expert, V_basis_expert = valuefunctiongenerator(alpha_vec, expert_policy, env)\n",
        "  \n",
        "  npp=0\n",
        "  while npp < 2:\n",
        "    \n",
        "    if npp ==0 :\n",
        "      rl_policy = p = {'1':'F', '2':'F', '3':'T', '4':'B',\n",
        "          '21':'F', '22':'F', '23':'F', '24':'B'}\n",
        "    \n",
        "    ## value function of RL policy\n",
        "    V_rl, V_basis_rl = valuefunctiongenerator(alpha_vec, rl_policy, env)\n",
        "\n",
        "    # print(V_basis_rl,V_basis_expert)\n",
        "    ## linaer programming\n",
        "    if V_expert - V_rl >= 0:\n",
        "      V_input += V_basis_rl - V_basis_expert\n",
        "    else:\n",
        "      V_input += penalty_factor* (V_basis_rl - V_basis_expert)\n",
        "    ## optimization problem  \n",
        "    #res = scipy.optimize.linprog(V_input, bounds=(-1,1), method=\"simplex\")\n",
        "    print('V_input',V_input)\n",
        "    #res = linprog(V_input, bounds=(0,1), method=\"simplex\")\n",
        "    #res = linprog(V_input.tolist(), A_ub=[[V_input[0],V_input[1]]], b_ub=0, bounds=(0,1), method=\"simplex\")\n",
        "\n",
        "    #alpha_vec_new = res.x\n",
        "    V_expert_new = np.dot(alpha_vec_new, V_basis_expert)\n",
        "    npp += 1\n",
        "    # print(res)\n",
        "    # print(res.x)\n"
      ],
      "metadata": {
        "id": "ctzPh-Up3_f8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}